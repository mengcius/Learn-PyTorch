{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.高阶操作.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengcius/pytorch-learn/blob/master/8_%E9%AB%98%E9%98%B6%E6%93%8D%E4%BD%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkonl0Pho924",
        "colab_type": "text"
      },
      "source": [
        "## 8.高阶操作\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLc-JJ50qje_",
        "colab_type": "text"
      },
      "source": [
        "### where 找到满足条件的\n",
        "torch.where(condition,input,other) 满足condition条件返回input，不满足返回other。\n",
        "\n",
        "因为直接赋值c[0]=a[0]是不规则的，for循环赋值使c[i,j]=a[i,j]是在CPU上没有加速的，而要用的where()是高度GPU并行实现的，一种逻辑时序控制。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGeugroOsq9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INd9IZ-3oyFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "95e509f4-5d9d-44a4-99f5-aeb0e213cd0b"
      },
      "source": [
        "x=torch.randn(3,2)\n",
        "y=torch.zeros(3,2)\n",
        "x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.2915, -0.8518],\n",
              "        [ 0.7951,  1.6197],\n",
              "        [-1.6230, -0.5936]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaLy0xrys23v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7e098126-cf7f-4219-e093-e227fa4264ff"
      },
      "source": [
        "torch.where(x>0.5,x,y) #大于0.5的元素取x中对应的元素"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000],\n",
              "        [0.7951, 1.6197],\n",
              "        [0.0000, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JMg82l2vqNy",
        "colab_type": "text"
      },
      "source": [
        "### gather 收集，查表\n",
        "torch.gather(input, dim, index, out=None, sparse_grad=False)  这是通过GPU完成的。\n",
        "\n",
        "新需求可以先看能否用Torch中的where/gather来实现，效率高点，如果不行再用python中实时控制慢慢的复制/选择等。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afEDmGOAtB2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f8dfe656-f075-489f-bb67-8f41ac9cf53d"
      },
      "source": [
        "t=torch.tensor([[6,7],[8,9]])\n",
        "t"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6, 7],\n",
              "        [8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn2MOqpGyhR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "34347da3-ba6b-4b8d-be68-db756b0a16e2"
      },
      "source": [
        "torch.gather(t,1,torch.tensor([[0,0,1],[1,0,1]])) #被查的表，dim=1，用索引查原表的值"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6, 6, 7],\n",
              "        [9, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT9Bgy6s5JZ-",
        "colab_type": "text"
      },
      "source": [
        "神经网络输出标签label对应了索引，如果将序号绑定到另一种序号，要查找到索引对应的label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNt44HRby0gD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "354e4664-888f-4731-e1a1-7db608c6d3ea"
      },
      "source": [
        "prob=torch.randn(4,10) #预测4张图片的minist类别结果\n",
        "idx=prob.topk(dim=1,k=3) #概率最大的前3种类别的索引\n",
        "idx"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([[1.9269, 1.6217, 1.4724],\n",
              "        [1.3163, 0.6043, 0.4540],\n",
              "        [1.6907, 1.4655, 1.2851],\n",
              "        [1.5658, 0.4750, 0.3527]]), indices=tensor([[4, 3, 1],\n",
              "        [3, 7, 0],\n",
              "        [6, 4, 9],\n",
              "        [4, 8, 3]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3LMq6YE6wv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ce8c8259-cfc7-44dc-b000-38432c45ace6"
      },
      "source": [
        "label=torch.arange(10)+100 #假设label序号重新设置了\n",
        "label.expand(4,10)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
              "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
              "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
              "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc67riUW8wIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c1d9d739-03e8-4895-9faa-9c13b542912a"
      },
      "source": [
        "torch.gather(label.expand(4,10),1,idx[1]) #重新查找到索引对应的label"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[104, 103, 101],\n",
              "        [103, 107, 100],\n",
              "        [106, 104, 109],\n",
              "        [104, 108, 103]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    }
  ]
}
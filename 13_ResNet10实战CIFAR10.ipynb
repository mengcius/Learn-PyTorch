{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_ResNet10实战CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengcius/pytorch-learn/blob/master/13_ResNet10%E5%AE%9E%E6%88%98CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOKEjDCSnhkN",
        "colab_type": "text"
      },
      "source": [
        "## 13_ResNet10实战CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZWT1j_xnj7J",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukZQeTQKnCea",
        "colab_type": "code",
        "outputId": "4f682224-8831-46ce-ecc8-db33b43548a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import  torch\n",
        "from    torch.utils.data import DataLoader\n",
        "from    torchvision import datasets\n",
        "from    torchvision import transforms\n",
        "\n",
        "batchsz = 128\n",
        "\n",
        "# 加载数据集\n",
        "cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "]), download=True)\n",
        "cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "]), download=True)\n",
        "cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "x, label = iter(cifar_train).next() # 一个batch\n",
        "print('x:', x.shape, 'label:', label.shape) # x: torch.Size([32, 3, 32, 32]) label: torch.Size([32])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 71928917.16it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "x: torch.Size([128, 3, 32, 32]) label: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd5ZbJg2n9_z",
        "colab_type": "text"
      },
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uTj3RTTn9uG",
        "colab_type": "code",
        "outputId": "90b56739-21b5-4b46-a042-bfe91ccc0ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "import  torch\n",
        "from    torch.utils.data import DataLoader\n",
        "from    torchvision import datasets\n",
        "from    torchvision import transforms\n",
        "from    torch import nn, optim\n",
        "\n",
        "batch_size=200\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('mnist_data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.RandomHorizontalFlip(), # 水平翻转，Random是随机做\n",
        "                       transforms.RandomVerticalFlip(), # 竖直翻转\n",
        "                       transforms.RandomRotation(15), # 随机旋转-15~15度之间任意角度\n",
        "                       transforms.RandomRotation([90, 270]), # 随机旋转这几个选项中的一个角度（只能有两个值）\n",
        "                       transforms.Resize([32, 32]), # 缩放\n",
        "                       transforms.RandomCrop([28, 28]), # 裁剪\n",
        "                       transforms.ToTensor(), # numpy格式转为Tensor\n",
        "                       # transforms.Normalize((0.1307,), (0.3081,)) # 归一化\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True) # 一次加载batch_size张图片，随机打散\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('mnist_data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x, label = iter(train_loader).next()\n",
        "print('x:', x.shape, 'label:', label.shape) #x: torch.Size([200, 1, 28, 28]) label: torch.Size([200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 16384/9912422 [00:00<01:10, 141179.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 31513003.04it/s]                          \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 462537.83it/s]\n",
            "  2%|▏         | 40960/1648877 [00:00<00:04, 401402.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 8370367.83it/s]                          \n",
            "8192it [00:00, 176438.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "x: torch.Size([200, 1, 28, 28]) label: torch.Size([200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo3i7FhsSjMx",
        "colab_type": "text"
      },
      "source": [
        "### lenet5.py\n",
        "10 loss: 0.799041211605072\n",
        "10 test acc: 0.6468\n",
        "\n",
        "30 loss: 0.38023799657821655\n",
        "30 test acc: 0.6309\n",
        "\n",
        "80 loss: 0.13082881271839142\n",
        "80 test acc: 0.601"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AaIFQ5uSjBA",
        "colab_type": "code",
        "outputId": "217463d4-7edb-4ea5-c438-798c9c42905d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import  torch\n",
        "from    torch import nn\n",
        "from    torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Lenet5(nn.Module): # 继承自nn.Module\n",
        "    \"\"\"\n",
        "    for cifar10 dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self): # 初始化\n",
        "        super(Lenet5, self).__init__()\n",
        "\n",
        "        # 卷积层部分\n",
        "        self.conv_unit = nn.Sequential( # 把网络结构包在Sequential里，方便用各层类\n",
        "            # x: [b, 3, 32, 32] =>\n",
        "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0), # 输入3通道，输出16通道\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0), # 或AvgPool2d\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            # 输出=>[b, 16, 5, 5]\n",
        "        )\n",
        "\n",
        "        # 自定义fc unit实现全连接层（flatten打平后作为它的输入）\n",
        "        self.fc_unit = nn.Sequential(\n",
        "            nn.Linear(16*5*5, 120), # 16*5*5是卷积层输出的尺寸\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10) # 10分类\n",
        "        )\n",
        "\n",
        "        # 计算卷积层输出的大小，以算出全连接层的输入大小\n",
        "        tmp = torch.randn(2, 3, 32, 32) # [b, 3, 32, 32]\n",
        "        out = self.conv_unit(tmp)\n",
        "        print('conv out:', out.shape) # [2, 16, 5, 5]\n",
        "\n",
        "        # 定义损失，可以在类外写，这里省略\n",
        "        # self.criteon = nn.CrossEntropyLoss() # 交叉熵损失，分类问题，包含了softmax\n",
        "        # self.criteon = nn.MSELoss() # 均方差损失，逼近、回归问题\n",
        "\n",
        "    def forward(self, x): # 前向运算，反向传播不用自己写\n",
        "        \"\"\"\n",
        "        :param x: [b, 3, 32, 32]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        batchsz = x.size(0) # 取batch size\n",
        "        x = self.conv_unit(x) # 卷积层部分，[b, 3, 32, 32] => [b, 16, 5, 5]\n",
        "        x = x.view(batchsz, 16*5*5) # 展平，[b, 16, 5, 5] => [b, 16*5*5]\n",
        "        logits = self.fc_unit(x) # 全连接层部分，[b, 16*5*5] => [b, 10] \n",
        "\n",
        "        # 计算损失，可以在类外写，这里省略，输入[b, 10]\n",
        "        # pred = F.softmax(logits, dim=1) # 因有了CrossEntropyLoss免写，在10的维度上做softmax\n",
        "        # loss = self.criteon(logits, y) # 计算损失\n",
        "\n",
        "        return logits # 返回\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    net = Lenet5() # 实例化Lenet5对象\n",
        "\n",
        "    tmp = torch.randn(2, 3, 32, 32)\n",
        "    out = net(tmp)\n",
        "    print('lenet out:', out.shape) # 计算整个网路输出的大小，[2, 10]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv out: torch.Size([2, 16, 5, 5])\n",
            "lenet out: torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyjRHnyNS8LN",
        "colab_type": "text"
      },
      "source": [
        "### retnet.py\n",
        "\n",
        "ResNet10(ResNet18的缩减版)\n",
        "\n",
        "残差块的通道按照论文上安排时，效果更好。但尺寸好像越大越好，可能cifar尺寸太小。\n",
        "\n",
        "1、正常残差块训练：\n",
        "\n",
        "10 loss: 0.1671239137649536\n",
        "10 test acc: 0.7388\n",
        "\n",
        "25 loss: 0.0216824971139431\n",
        "25 test acc: 0.7451\n",
        "\n",
        "80 loss: 0.0015973985427990556\n",
        "80 test acc: 0.7414\n",
        "\n",
        "100 loss: 0.0008557319524697959\n",
        "100 test acc: 0.7452\n",
        "\n",
        "2、残差块尺寸修改，self.blk4 = ResBlk(256, 512, stride=1)：\n",
        "\n",
        "9 loss: 0.16980847716331482\n",
        "9 test acc: 0.7335\n",
        "\n",
        "31 loss: 0.03963826224207878\n",
        "31 test acc: 0.7496\n",
        "\n",
        "39 loss: 0.006199670024216175\n",
        "39 test acc: 0.7516\n",
        "\n",
        "100 loss: 0.00445919344201684\n",
        "100 test acc: 0.7539\n",
        "\n",
        "3、正常残差块+数据增强（裁剪和水平翻转）：\n",
        "\n",
        "11 loss: 0.6354655623435974\n",
        "11 test acc: 0.8089\n",
        "\n",
        "30 loss: 0.22230911254882812\n",
        "30 test acc: 0.8447\n",
        "\n",
        "50 loss: 0.18418802320957184\n",
        "50 test acc: 0.8512\n",
        "\n",
        "4、残差块尺寸全修改为stride=1+数据增强（裁剪和水平翻转）:\n",
        "\n",
        "11 loss: 0.5003647804260254\n",
        "11 test acc: 0.8295\n",
        "\n",
        "16 loss: 0.5709465742111206\n",
        "16 test acc: 0.8421\n",
        "\n",
        "22 loss: 0.24603290855884552\n",
        "22 test acc: 0.8581\n",
        "\n",
        "31 loss: 0.26374727487564087\n",
        "31 test acc: 0.8662\n",
        "\n",
        "50 loss: 0.1871185153722763\n",
        "50 test acc: 0.8702\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqHTyfE6Si9B",
        "colab_type": "code",
        "outputId": "3464d253-fac2-48a2-cc70-87de3c488b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "import  torch\n",
        "from    torch import  nn\n",
        "from    torch.nn import functional as F\n",
        "\n",
        "\n",
        "# 残差基本单元BasicBlock（适用于浅层网络，中间有两层卷积层）\n",
        "class ResBlk(nn.Module):\n",
        "    \"\"\"\n",
        "    resnet block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ch_in, ch_out, stride=1):\n",
        "        \"\"\"\n",
        "        :param ch_in:\n",
        "        :param ch_out:\n",
        "        \"\"\"\n",
        "        super(ResBlk, self).__init__()\n",
        "\n",
        "        # 中间卷积层部分，2层，we add stride support for resbok, which is distinct from tutorials.\n",
        "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=stride, padding=1) # 第一层，能升维就升维，stride变量\n",
        "        self.bn1 = nn.BatchNorm2d(ch_out) # 和上层的通道ch_out一样\n",
        "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1) # 这层尺寸不变\n",
        "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
        "\n",
        "        # 确保跳连的输出通道是ch_out，以便与中间卷积层部分逐像素相加（通道和尺寸一样）\n",
        "        self.extra = nn.Sequential()\n",
        "        if ch_out != ch_in: # shortcut的ch_in和ch_out维度不匹配时\n",
        "            # [b, ch_in, h, w] => [b, ch_out, h, w]\n",
        "            self.extra = nn.Sequential(\n",
        "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=stride), # 1*1卷积核来升维或降维，stride使尺寸同样变化\n",
        "                nn.BatchNorm2d(ch_out)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [b, ch, h, w]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # 中间卷积部分\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out)) # 可不加F.relu\n",
        "        # print('conv:', out.shape)\n",
        "        # print('short:', self.extra(x).shape)\n",
        "       \n",
        "        # 跳连并element-wise add\n",
        "        out = self.extra(x) + out # 需要输入ch_in与输出ch_out大小相等\n",
        "        out = F.relu(out)\n",
        "        # print('ResBlk:', out.shape)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet10，ResNet18的缩减版\n",
        "class ResNet10(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResNet10, self).__init__()\n",
        "\n",
        "        # 预处理卷积层\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=3, padding=0),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "        # 残差块，followed 4 blocks\n",
        "        # [b, 64, h, w] => [b, 128, h ,w]\n",
        "        self.blk1 = ResBlk(64, 64, stride=1) # 注意当ch_in=ch_out时，stride设为1，因为此时跳连不能做缩放尺寸\n",
        "        # [b, 128, h, w] => [b, 256, h, w]\n",
        "        self.blk2 = ResBlk(64, 128, stride=1) # 调用ResBlk，输入通道，输出通道，\n",
        "        # [b, 256, h, w] => [b, 512, h, w]\n",
        "        self.blk3 = ResBlk(128, 256, stride=1)\n",
        "        # [b, 512, h, w] => [b, 512, h, w]，h w是变化的\n",
        "        self.blk4 = ResBlk(256, 512, stride=1) \n",
        "\n",
        "        self.outlayer = nn.Linear(512*1*1, 10) # 全连接层，要先池化成1*1后输入，10分类\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # 预处理卷积层，[b, 3, h, w] => [b, 64, h*, w*]\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # 4组残差块，即4*2层卷积层，[b, 64, h*, w*] => [b, 512, h`, w`]\n",
        "        x = self.blk1(x)\n",
        "        x = self.blk2(x)\n",
        "        x = self.blk3(x)\n",
        "        x = self.blk4(x)\n",
        "        # print('after res conv:', x.shape) # 打印输出大小，[b, 512, 2, 2]\n",
        "\n",
        "        # 池化层，[b, 512, h`, w`] => [b, 512, 1, 1]\n",
        "        x = F.adaptive_avg_pool2d(x, [1, 1]) # 等价于在int里的类写法self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # print('after pool:', x.shape) # [2, 512, 1, 1]\n",
        "        # print('size:', x.size(0)) # 2\n",
        "\n",
        "        # 全连接层\n",
        "        x = x.view(x.size(0), -1) # 拉平c h w，x.size(0)=2\n",
        "        # print('after view:', x.shape) # [2, 512]\n",
        "\n",
        "        x = self.outlayer(x) \n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # 测试ResBlk的输出大小\n",
        "    blk = ResBlk(64, 128, stride=2)\n",
        "    tmp = torch.randn(2, 64, 32, 32)\n",
        "    out = blk(tmp)\n",
        "    print('Resblock:', out.shape) # [2, 128, 16, 16]，尺寸减少参数减少\n",
        "\n",
        "    # 测试ResNet的输出大小\n",
        "    x = torch.randn(2, 3, 32, 32) # ResNet原始版本是正式图片大小224*224，输入大小改变时网络结构也要重新设计\n",
        "    model = ResNet10()\n",
        "    out = model(x)\n",
        "    print('resnet:', out.shape) # [2, 10]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resblock: torch.Size([2, 128, 16, 16])\n",
            "after pool: torch.Size([2, 512, 1, 1])\n",
            "size: 2\n",
            "after view: torch.Size([2, 512])\n",
            "resnet: torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucg9N03XaB3g",
        "colab_type": "text"
      },
      "source": [
        "### main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrei2gL3Si3y",
        "colab_type": "code",
        "outputId": "38930161-4739-45db-ff88-78521dd68b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import  torch\n",
        "from    torch.utils.data import DataLoader\n",
        "from    torchvision import datasets\n",
        "from    torchvision import transforms\n",
        "from    torch import nn, optim\n",
        "\n",
        "# from    lenet5 import Lenet5\n",
        "# from    resnet import ResNet10\n",
        "\n",
        "\n",
        "def main():\n",
        "    batchsz = 128\n",
        "\n",
        "    # 加载数据集\n",
        "    cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # 数据归一化，R G B上的标准差\n",
        "                             std=[0.229, 0.224, 0.225]) # R G B上的方标准差\n",
        "    ]), download=True)\n",
        "    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "    cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ]), download=True)\n",
        "    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "    x, label = iter(cifar_train).next() # 一个batch\n",
        "    print('x:', x.shape, 'label:', label.shape)\n",
        "    # x:torch.Size([32,3,32,32]) label:torch.Size([32])\n",
        "\n",
        "\n",
        "    # 调用构建网络实例\n",
        "    device = torch.device('cuda') # GPU\n",
        "    # model = Lenet5().to(device)\n",
        "    model = ResNet10().to(device)\n",
        "    print(model) # 打印结构（初始化类里的结构，forward里打印不出来）\n",
        "\n",
        "    criteon = nn.CrossEntropyLoss().to(device) # 定义交叉熵损失，分类问题，包含了softmax\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3) # 定义优化器，传入网络参数\n",
        "\n",
        "\n",
        "    for epoch in range(200):\n",
        "\n",
        "        # 训练\n",
        "        model.train() # 有些模型在train和eval时计算不同，要声明什么模式，如BN、Dropout\n",
        "        for batchidx, (x, label) in enumerate(cifar_train): # 迭代\n",
        "            x, label = x.to(device), label.to(device) # GPU，x:[b, 3, 32, 32]，label:[b]\n",
        "\n",
        "            logits = model(x) # 调用model的forward，传进x\n",
        "            # logits:[b, 10]，label:[b]\n",
        "            loss = criteon(logits, label) # 计算loss，输入是logits而不是经过softmax的predect\n",
        "\n",
        "            # backprop\n",
        "            optimizer.zero_grad() # 梯度清零，否则默认累加\n",
        "            loss.backward() # 计算梯度，累加到了0上面\n",
        "            optimizer.step() # 沿梯度更新\n",
        "\n",
        "        # 这打印的是此epoch中最后一个batch的loss，loss数据仅供参考，主要看TEST ACC\n",
        "        print(epoch, 'loss:', loss.item()) # loss是tensor标量，要通过item转为numpy打印出来\n",
        "\n",
        "        # 测试\n",
        "        model.eval() # eval模式\n",
        "        with torch.no_grad(): # 测试时不需要backprop计算梯度，不需要构建计算图，防止打乱计算图更安全\n",
        "            # test\n",
        "            total_correct = 0 # 全局\n",
        "            total_num = 0\n",
        "            for x, label in cifar_test:\n",
        "                x, label = x.to(device), label.to(device)\n",
        "              \n",
        "                logits = model(x) # [b, 10]\n",
        "                pred = logits.argmax(dim=1) # 对logits做argmax得到预测标签，[b]\n",
        "\n",
        "                # [b] vs [b] => scalar tensor byteTensor => float，再累加，转为numpy\n",
        "                total_correct += torch.eq(pred, label).float().sum().item() # 预测与真实标签逐元素比较\n",
        "                total_num += x.size(0) # batch数累加\n",
        "\n",
        "            acc = total_correct / total_num # 测试集总的准确率\n",
        "            print(epoch, 'test acc:', acc)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "x: torch.Size([128, 3, 32, 32]) label: torch.Size([128])\n",
            "ResNet10(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(3, 3))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (blk1): ResBlk(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential()\n",
            "  )\n",
            "  (blk2): ResBlk(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blk3): ResBlk(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blk4): ResBlk(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (outlayer): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "0 loss: 1.2650936841964722\n",
            "0 test acc: 0.5581\n",
            "1 loss: 1.1712342500686646\n",
            "1 test acc: 0.6274\n",
            "2 loss: 0.9182676076889038\n",
            "2 test acc: 0.6566\n",
            "3 loss: 1.0661170482635498\n",
            "3 test acc: 0.7176\n",
            "4 loss: 0.7210609316825867\n",
            "4 test acc: 0.7497\n",
            "5 loss: 0.515949547290802\n",
            "5 test acc: 0.7422\n",
            "6 test acc: 0.7864\n",
            "7 loss: 0.3919241428375244\n",
            "7 test acc: 0.793\n",
            "8 loss: 0.5013679265975952\n",
            "8 test acc: 0.8101\n",
            "9 loss: 0.602636992931366\n",
            "9 test acc: 0.8058\n",
            "10 loss: 0.46631938219070435\n",
            "10 test acc: 0.8089\n",
            "11 loss: 0.5003647804260254\n",
            "11 test acc: 0.8295\n",
            "12 loss: 0.25595635175704956\n",
            "12 test acc: 0.8352\n",
            "13 loss: 0.42832058668136597\n",
            "13 test acc: 0.8318\n",
            "14 loss: 0.36918073892593384\n",
            "14 test acc: 0.83\n",
            "15 loss: 0.32142677903175354\n",
            "15 test acc: 0.837\n",
            "16 loss: 0.5709465742111206\n",
            "16 test acc: 0.8421\n",
            "17 loss: 0.5539027452468872\n",
            "17 test acc: 0.8482\n",
            "18 loss: 0.36464759707450867\n",
            "18 test acc: 0.8515\n",
            "19 loss: 0.38161930441856384\n",
            "19 test acc: 0.8374\n",
            "20 loss: 0.3017630875110626\n",
            "20 test acc: 0.8513\n",
            "21 loss: 0.1647506207227707\n",
            "21 test acc: 0.8492\n",
            "22 loss: 0.24603290855884552\n",
            "22 test acc: 0.8581\n",
            "23 loss: 0.18332362174987793\n",
            "23 test acc: 0.8502\n",
            "24 loss: 0.32948988676071167\n",
            "24 test acc: 0.8564\n",
            "25 loss: 0.16701728105545044\n",
            "25 test acc: 0.8575\n",
            "26 loss: 0.18929153680801392\n",
            "26 test acc: 0.8647\n",
            "27 loss: 0.24675831198692322\n",
            "27 test acc: 0.8594\n",
            "28 loss: 0.200476735830307\n",
            "28 test acc: 0.8537\n",
            "29 loss: 0.15564212203025818\n",
            "29 test acc: 0.8603\n",
            "30 loss: 0.11263607442378998\n",
            "30 test acc: 0.8596\n",
            "31 loss: 0.26374727487564087\n",
            "31 test acc: 0.8662\n",
            "32 loss: 0.24866247177124023\n",
            "32 test acc: 0.8628\n",
            "33 loss: 0.12930741906166077\n",
            "33 test acc: 0.8607\n",
            "34 loss: 0.09740503132343292\n",
            "34 test acc: 0.8621\n",
            "35 loss: 0.14432422816753387\n",
            "35 test acc: 0.8639\n",
            "36 loss: 0.09658964723348618\n",
            "36 test acc: 0.8679\n",
            "37 loss: 0.06666876375675201\n",
            "37 test acc: 0.8666\n",
            "38 loss: 0.2137000560760498\n",
            "38 test acc: 0.8583\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}